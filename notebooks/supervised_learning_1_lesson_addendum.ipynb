{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afc4016",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Data Science Intersection\n",
    "How do we learn Machine Learning?\n",
    "\n",
    "<img src=\"../assets/dsintersect.jpg\" width=\"50%\" alt=\"Data Science Intersection\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d4f32",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- A statistical / machine learning model for regression.\n",
    "- Model the relationship between a dependent variable and one or more independent variables.\n",
    "- Equation: $$ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon $$\n",
    "  - Dependent variable: $$ Y $$\n",
    "  - Independent variables: $$ X_1, X_2, \\ldots, X_n $$\n",
    "  - Coefficients: $$ \\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n $$\n",
    "  - Error term: $$ \\epsilon $$\n",
    "  ![linear_reg](../assets/linear_reg.jpeg)\n",
    " \n",
    "- Assumptions: \n",
    "  - Linear relationship between dependent and independent variables\n",
    "  - No multicollinearity among independent variables\n",
    "  - Homoscedasticity: Constant variance of errors\n",
    "  - Normal distribution of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a164ba90",
   "metadata": {},
   "source": [
    "# Let's get an intuition of the linear regression process\n",
    "[Click here for external link](https://developers.google.com/machine-learning/crash-course/linear-regression/parameters-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04667e04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Linear Regression Implementation Example\n",
    "Below is a complete example of implementing linear regression with scikit-learn:\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df.dropna()  # Handle missing values\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train model (minimizes MSE loss function)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Display model coefficients\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3d389",
   "metadata": {},
   "source": [
    "## Training / Fitting the Model\n",
    "\n",
    "### Cost function\n",
    "\n",
    "- Definition: A measure of how far off our predictions are from the actual values.\n",
    "\n",
    "- Mean Squared Error (MSE): $$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n",
    "  - Actual value: $$y_i$$\n",
    "  - Predicted value: $$ \\hat{y_i} $$\n",
    "  - Number of observations: $$ n $$\n",
    "\n",
    "### Objective\n",
    "\n",
    "- The goal is to find the values of the coefficients that minimize the sum of the squared differences between the observed values and the values predicted by the linear equation.\n",
    "    - **Ordinary Least Squares (OLS)**: OLS is the most common method used in linear regression to estimate the coefficients. Often involves solving a set of linear equations or using optimization algorithms.\n",
    "    - **Gradient Descent**: An optimization algorithm used to minimize the cost function by iteratively moving towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3310a",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "### R-Squared (R²) in Linear Regression\n",
    "\n",
    "R-squared, often denoted as R², is a statistical measure that represents the proportion of the variance for the dependent variable that's explained by the independent variables in a regression model.\n",
    "\n",
    "#### Definition and Formula\n",
    "\n",
    "R² is defined as the ratio of the variance explained by the model to the total variance. It is calculated as:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\text{Sum of Squares of Residuals (SSR)}}{\\text{Total Sum of Squares (SST)}} $$\n",
    "\n",
    "where,\n",
    "- SSR (Sum of Squares of Residuals) is the sum of the squares of the model residuals.\n",
    "- SST (Total Sum of Squares) is the total sum of the squares of the difference from the mean.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **Value Range**: R² ranges from 0 to 1.\n",
    "- **Higher R²**: A higher R² indicates a better fit of the model. An R² of 1 means that the regression predictions perfectly fit the data.\n",
    "- **Limitation**: While a high R² indicates a good fit, it doesn’t guarantee that the model is appropriate. Overfitting, where the model is too complex, can lead to misleadingly high R² values.\n",
    "\n",
    "![sst_ssr](../assets/sst_ssr.png)\n",
    "\n",
    "### Sum of Squares in Linear Regression\n",
    "\n",
    "Sum of Squares is a measure used in statistical analysis to quantify variations in data points.\n",
    "\n",
    "#### 1. Total Sum of Squares (SST)\n",
    "\n",
    "SST measures the total variation in the dependent variable. It is calculated as:\n",
    "\n",
    "$$ \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $$\n",
    "\n",
    "where $ y_i $ is the actual value, $ \\bar{y} $ is the mean of the dependent variable, and $ n $ is the number of observations.\n",
    "\n",
    "#### 2. Sum of Squares of Residuals (SSR)\n",
    "\n",
    "SSR, also known as the sum of squared errors of the model, measures the unexplained variation by the model. It is calculated as:\n",
    "\n",
    "$$ \\text{SSR} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $ \\hat{y}_i $ is the predicted value.\n",
    "\n",
    "#### Importance in Regression Analysis\n",
    "\n",
    "- **SST**: Reflects the total variation in the data.\n",
    "- **SSR**: Indicates the amount of variation not explained by the model.\n",
    "- **Balance**: A good model minimizes SSR and explains most of the SST.\n",
    "\n",
    "Sum of Squares helps in understanding the effectiveness of the regression model in explaining the variance of the data. The lower the SSR in comparison to SST, the better the model's predictive capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95d29a",
   "metadata": {},
   "source": [
    "## Scikit-Learn Tutorial\n",
    "\n",
    "Let's implement linear regression in Python using the Scikit-Learn (`sklearn`) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5abb4",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154fe7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Array of 100 values with mean = 1.5, stddev = 2.5\n",
    "X = 2.5 * np.random.randn(100) + 1.5\n",
    "# Generate 100 residual terms\n",
    "res = 0.5 * np.random.randn(100)\n",
    "# Actual values of Y (equation)\n",
    "y = 2 + 0.3 * X + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49866524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y into a pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'X': X,\n",
    "    'y': y\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's often useful to visualize the data before building models.\n",
    "plt.scatter(df['X'], df['y'])\n",
    "plt.title('Dataset')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088044c6",
   "metadata": {},
   "source": [
    "Before we train (fit) the model, we'll need to split the dataset into `training` and `testing` sets. The split is an `80:20` (which can be arbitrary).\n",
    "\n",
    "The reason for splitting the dataset is to avoid overfitting. Overfitting occurs when the model learns the training data too well, including the noise, and is unable to generalize to new  / unseen data. We will fit the model on the training set and evaluate it on the testing set (unseen data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['X'], df['y'], test_size=0.2, random_state=0)\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a461bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca87f20",
   "metadata": {},
   "source": [
    "Now, let's train (fit) the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02980042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the test set.\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8575fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set using metrics like Mean Squared Error (MSE) and R-squared.\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfe4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfdc5d9",
   "metadata": {},
   "source": [
    "## Statsmodels Tutorial\n",
    "\n",
    "We can also implement linear regression using the `statsmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d55b0",
   "metadata": {},
   "source": [
    "Prepare the predictor and response variables, for statsmodels we need to add a constant term to the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['X']\n",
    "y = df['y']\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f80f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922f8c7",
   "metadata": {},
   "source": [
    "Statsmodels provides extensive statistical diagnostics and tests about the model, such as _p-values, confidence intervals, and in-depth analysis of residuals._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "# Regular R² always increases when adding more predictors, even if they're irrelevant\n",
    "# Adjusted R² penalizes adding unnecessary predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the test set.\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9622bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set using metrics like Mean Squared Error (MSE) and R-squared.\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22354ab",
   "metadata": {},
   "source": [
    "The results are the same as sklearn's model above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a7741",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now, use the statsmodels library to perform linear regression on the \"diamonds\" dataset, which contains the prices and attributes of almost 54,000 diamonds. We'll try to predict the price of a diamond based on its carat weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016449e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e81182",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diamonds['carat']\n",
    "y = diamonds['price']\n",
    "# Adds a constant term to the predictor\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use 30% for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39229800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfecfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5555596",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test['carat'], y_test, alpha=0.3, label='Actual Price')\n",
    "plt.plot(X_test['carat'], y_pred, color='red', label='Predicted Price')\n",
    "plt.title('Diamond Price Prediction')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96819c4a",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "- A statistical / machine learning model for classification, specifically binary (`positive or 1` class vs `negative or 0` class) classification.\n",
    "- Model the probability that a given input point belongs to the positive class (negative probability is just `1 - positive probability`)\n",
    "- Equation: $$ P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}} $$\n",
    "  - Uses the `logistic function (Sigmoid function)` to model the linear predicted output (from a linear regression) into a binary output variable.\n",
    "  - The logistic function converts any input to a value between 0 and 1, which is interpreted as a probability of the instance belonging to the positive class.\n",
    "  ![log_reg](../assets/log_reg.png)\n",
    "\n",
    "- Concepts:\n",
    "  - Odds: $$ \\frac{P(Y=1)}{1 - P(Y=1)} $$\n",
    "  - In the context of probability, \"odds\" is a way of expressing the likelihood that a particular event will occur. Odds are calculated as the ratio of the probability of the event occurring to the probability of the event not occurring.\n",
    "  - Log-Odds: $$ \\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1X $$\n",
    "  - Log odds, or the logit function, is the logarithm of the odds. In logistic regression, the logit function is used to create a linear combination of the independent variables.\n",
    "  - The logistic function is the inverse of the logit function. It takes the log odds and transforms them back into a probability.\n",
    "\n",
    "- Example: Binary classification tasks such as email spam detection, image recognition, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cc613",
   "metadata": {},
   "source": [
    "## Training / Fitting the Model\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "- Objective: Find the values of coefficients that maximize the `likelihood (function)` of observing the given sample.\n",
    "- Likelihood Function: Measures how well our model explains the observed data.\n",
    "- Procedure:\n",
    "  1. Define the likelihood function for the logistic regression model.\n",
    "  2. Use optimization techniques (e.g., gradient descent) to maximize the likelihood function.\n",
    "  3. The values of coefficients that maximize the likelihood function are considered as the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b71daa",
   "metadata": {},
   "source": [
    "## Scikit-Learn Tutorial\n",
    "For this tutorial, we'll use the Iris dataset, which is a classic dataset for classification. The dataset contains 150 observations of iris flowers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f70a2",
   "metadata": {},
   "source": [
    "![Three species of iris: setosa, versicolor, and virginica](../assets/Three-species-of-IRIS-flower.jpg)\n",
    "\n",
    "In this exercise, we will build a classifier to distinguish between two species of iris flowers: **Iris sentosa and Iris versicolor**. Using the measurements of sepal length, sepal width, petal length, and petal width, we'll train a model to accurately identify which species a flower belongs to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a69c4",
   "metadata": {},
   "source": [
    "We'll need to convert the problem to a binary classification for logistic regression. Drop the additional class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e23976",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[y != 2]\n",
    "y = y[y != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7045fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc530118",
   "metadata": {},
   "source": [
    "We'll use `Accuracy Score` to evaluate the performance of the model.\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Observations}} $$\n",
    "\n",
    "> There are many more metrics to evaluate classification models, which we'll cover in the next unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a187730",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e18842",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred)\n",
    "plt.xlabel(f'{iris.feature_names[0]}')\n",
    "plt.ylabel(f'{iris.feature_names[1]}')\n",
    "plt.title('Logistic Regression Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a81095",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 2], X_test[:, 3], c=y_pred)\n",
    "plt.xlabel(f'{iris.feature_names[2]}')\n",
    "plt.ylabel(f'{iris.feature_names[3]}')\n",
    "plt.title('Logistic Regression Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55877612",
   "metadata": {},
   "source": [
    "## Statsmodels Tutorial\n",
    "\n",
    "We can also implement logistic regression using the `statsmodels` library.\n",
    "\n",
    "Again, the benefits of statsmodels is it provides extensive statistical diagnostics and tests about the model, such as _p-values, confidence intervals, and in-depth analysis of residuals._\n",
    "\n",
    "Let's use another more realistic dataset such as the _Pima Indians Diabetes_ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8023c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
    "pima = pd.read_csv(url, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7012fc",
   "metadata": {},
   "source": [
    "Let's try to predict the diabetes outcome using glucose, BMI and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Glucose', 'BMI', 'Age']\n",
    "X = pima[features]\n",
    "y = pima['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a constant term to the predictor\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ae6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ed207",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbcf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe392028",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prob is an array of probabilities, we need to convert it to predicted class\n",
    "y_pred = np.array([1 if x > 0.5 else 0 for x in y_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d01a0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# K-Nearest Neighbors (K-NN)\n",
    "- A machine learning model for regression and classification.\n",
    "- Algorithm:\n",
    "  1. Compute the distance between the test data point and each of the training data points.\n",
    "  2. Identify the 'k' nearest data points to the test data point.\n",
    "  3. Perform a majority vote (for classification) or average (for regression) of these 'k' nearest points.\n",
    "- Distance Metrics:\n",
    "  - Euclidean Distance\n",
    "  - Manhattan Distance\n",
    "  - Minkowski Distance\n",
    " \n",
    "**Euclidean Distance in coordinate notation**\n",
    "![Euclidean Distance (in coordinate space)](../assets/distance.png)\n",
    "\n",
    "**Euclidean distance in vector notation**\n",
    "The Euclidean distance between two points in Euclidean space is a measure of the length of a straight line between these two points. In a space with $ p $ dimensions, the Euclidean distance between two points, $ P $ and $ Q $, with coordinates $ P = (p_1, p_2, \\ldots, p_p) $ and $ Q = (q_1, q_2, \\ldots, q_p) $, is given by the formula:\n",
    "\n",
    "$$ d(P, Q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\cdots + (p_p - q_p)^2} $$\n",
    "\n",
    "In a more generalized and compact form, it can be written as:\n",
    "\n",
    "$$ d(P, Q) = \\sqrt{\\sum_{i=1}^{p} (p_i - q_i)^2} $$\n",
    "\n",
    "where $ \\sum $ denotes the summation and $ \\sqrt{} $ denotes the square root.\n",
    "\n",
    "> We'll dive deeper into `Manhattan` and `Minkowski` Distance in Unit 3.6: Unsupervised Learning.\n",
    "- Example: If k=3, and the three closest points to a test instance are of classes A, A, and B, the algorithm would classify the test instance as class A.\n",
    "\n",
    "![knn](../assets/knn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0397",
   "metadata": {},
   "source": [
    "Choosing the Right 'k' in K-NN\n",
    "- Small 'k' value:\n",
    "  - Pros: Captures the nuances in the data.\n",
    "  - Cons: Sensitive to noise in the data; higher variance.\n",
    "- Large 'k' value:\n",
    "  - Pros: More stable and robust to noise.\n",
    "  - Cons: May oversimplify the model; higher bias.\n",
    "\n",
    "> We'll dive deeper into this in Unit 3.5: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e843c",
   "metadata": {},
   "source": [
    "## Scikit-Learn Tutorial\n",
    "\n",
    "Let's apply KNN on the `diamonds` regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9009dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diamonds['carat']\n",
    "y = diamonds['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd87f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa16380",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
    "knn_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333937ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54db1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Actual Prices vs Predicted Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0e53",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We can experiment with different `k` values and see how they affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a7451",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    errors.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Plotting\n",
    "plt.plot(range(1, 21), errors)\n",
    "plt.title('k-NN Varying number of neighbors (k)')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e621eaa",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now, let's practice on the `iris` dataset for classification, without dropping class `2`. Hence, this will be a multi-class (>2) classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e781e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1700e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c9186",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Let's revisit Linear regression for the diamond dataset\n",
    "Addressing OLS assumptions and potential issues (non-normal residuals, heteroscedasticity, and model specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8320d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "#diamonds = sns.load_dataset('diamonds')\n",
    "#diamonds.head()\n",
    "X = diamonds['carat']\n",
    "y = diamonds['price']\n",
    "# Adds a constant term to the predictor\n",
    "X = sm.add_constant(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model = sm.OLS(y_train, X_train).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1afb5",
   "metadata": {},
   "source": [
    "### 1. Residual normality diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50745ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model.resid\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "ax[0].set_title('Residual Histogram')\n",
    "qqplot(residuals, line='s', ax=ax[1])\n",
    "ax[1].set_title('Q-Q Plot')\n",
    "\n",
    "jb_stat, jb_pvalue, skew, kurt = jarque_bera(residuals)\n",
    "print(f\"Jarque–Bera: stat={jb_stat:.2f}, p-value={jb_pvalue:.5f}, skew={skew:.3f}, kurtosis={kurt:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9138601",
   "metadata": {},
   "source": [
    "### 2. Heteroscedasticity test & robust standard errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bp_pvalue, _, _ = het_breuschpagan(residuals, model.model.exog)\n",
    "print(f\"Breusch–Pagan p-value: {bp_pvalue:.5f}\")\n",
    "\n",
    "robust_model = model.get_robustcov_results(cov_type='HC3')\n",
    "print(robust_model.summary())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(model.fittedvalues, residuals, alpha=0.3)\n",
    "plt.axhline(0, color='red', lw=1)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90ff62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 3. Improved model: add quadratic term for carat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'carat2' not in diamonds.columns:\n",
    "    diamonds['carat2'] = diamonds['carat'] ** 2\n",
    "X_poly = diamonds[['carat', 'carat2']]\n",
    "X_poly = sm.add_constant(X_poly)\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c491977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the polynomial model\n",
    "poly_model = sm.OLS(y_train_poly, X_train_poly).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd765c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Residual Normality Diagnostics\n",
    "residuals_poly = poly_model.resid\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(residuals_poly, kde=True, ax=ax[0])\n",
    "ax[0].set_title('Residual Histogram (Polynomial Model)')\n",
    "qqplot(residuals_poly, line='s', ax=ax[1])\n",
    "ax[1].set_title('Q-Q Plot (Polynomial Model)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Jarque-Bera test for normality\n",
    "jb_stat, jb_pvalue, skew, kurt = jarque_bera(residuals_poly)\n",
    "print(f\"\\nJarque–Bera Test (Polynomial Model):\")\n",
    "print(f\"  Statistic: {jb_stat:.2f}, p-value: {jb_pvalue:.5f}\")\n",
    "print(f\"  Skewness: {skew:.3f}, Kurtosis: {kurt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Heteroscedasticity Test\n",
    "_, bp_pvalue, _, _ = het_breuschpagan(residuals_poly, poly_model.model.exog)\n",
    "print(f\"\\nBreusch–Pagan Test (Polynomial Model): p-value = {bp_pvalue:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Robust Standard Errors\n",
    "robust_poly = poly_model.get_robustcov_results(cov_type='HC3')\n",
    "print(\"\\nPolynomial Model with Robust Standard Errors (HC3):\")\n",
    "print(robust_poly.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebe953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Model Performance\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "print('\\nModel Performance:')\n",
    "print(f\"MSE: {mean_squared_error(y_test_poly, y_pred_poly):.2f}\")\n",
    "print(f\"R²: {r2_score(y_test_poly, y_pred_poly):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Residuals vs Fitted Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(poly_model.fittedvalues, residuals_poly, alpha=0.3)\n",
    "plt.axhline(0, color='red', lw=1)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted (Polynomial Model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a78f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Model Assessment\n",
    "\n",
    "Comparing our linear and polynomial models:\n",
    "\n",
    "- The polynomial model better captures the non-linear relationship between carat and price\n",
    "- Both models still show non-normal residuals and heteroscedasticity\n",
    "- The quadratic term improves the model fit as seen in the R-squared values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bab520",
   "metadata": {},
   "source": [
    "### 4. Log-Transformed Model to Address Non-Normality and Heteroscedasticity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the relationship between carat and price in the log-transformed space.\n",
    "# Create log-transformed variables\n",
    "diamonds['log_price'] = np.log1p(diamonds['price'])\n",
    "diamonds['log_carat'] = np.log(diamonds['carat'])\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original scale\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(diamonds['carat'], diamonds['price'], alpha=0.3)\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price vs Carat (Original Scale)')\n",
    "\n",
    "# Log-transformed scale\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(diamonds['log_carat'], diamonds['log_price'], alpha=0.3, color='orange')\n",
    "plt.xlabel('log(Carat)')\n",
    "plt.ylabel('log(Price)')\n",
    "plt.title('log(Price) vs log(Carat)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74633947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the relationship between carat and price in the log-transformed space.\n",
    "# Create log-transformed variables\n",
    "diamonds['log_price'] = np.log1p(diamonds['price'])\n",
    "diamonds['log_carat'] = np.log(diamonds['carat'])\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original scale\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(diamonds['carat'], diamonds['price'], alpha=0.3)\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price vs Carat (Original Scale)')\n",
    "\n",
    "# Log-transformed scale\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(diamonds['log_carat'], diamonds['log_price'], alpha=0.3, color='orange')\n",
    "plt.xlabel('log(Carat)')\n",
    "plt.ylabel('log(Price)')\n",
    "plt.title('log(Price) vs log(Carat)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128cd0fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Prepare data for log model\n",
    "X_log = diamonds[['log_carat']]\n",
    "X_log = sm.add_constant(X_log)\n",
    "y_log = diamonds['log_price']\n",
    "\n",
    "# Split the data\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
    "    X_log, y_log, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Fit the log model\n",
    "log_model = sm.OLS(y_train_log, X_train_log).fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6665d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Residual Normality Diagnostics\n",
    "residuals_log = log_model.resid\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(residuals_log, kde=True, ax=ax[0])\n",
    "ax[0].set_title('Residual Histogram (Log Model)')\n",
    "qqplot(residuals_log, line='s', ax=ax[1])\n",
    "ax[1].set_title('Q-Q Plot (Log Model)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Heteroscedasticity Test\n",
    "_, bp_pvalue_log, _, _ = het_breuschpagan(residuals_log, log_model.model.exog)\n",
    "print(f\"\\nBreusch–Pagan Test (Log Model): p-value = {bp_pvalue_log:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c. Model Performance\n",
    "y_pred_log = log_model.predict(X_test_log)\n",
    "\n",
    "\n",
    "# Original metrics in log scale\n",
    "# log_mse_logscale = mean_squared_error(y_test_log, y_pred_log)\n",
    "# log_r2_logscale = r2_score(y_test_log, y_pred_log)\n",
    "\n",
    "# Metrics in original price scale for fair comparison\n",
    "y_test_original = np.expm1(y_test_log)\n",
    "y_pred_original = np.expm1(y_pred_log)\n",
    "log_mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "log_r2 = r2_score(y_test_original, y_pred_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Residuals vs Fitted Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(log_model.fittedvalues, residuals_log, alpha=0.3)\n",
    "plt.axhline(0, color='red', lw=1)\n",
    "plt.xlabel('Fitted Values (log scale)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted (Log Model)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Comparison Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "# Calculate RMSE for each model\n",
    "rmse_simple = mean_squared_error(y_test, model.predict(X_test), squared=False)\n",
    "rmse_quad = mean_squared_error(y_test_poly, y_pred_poly, squared=False)\n",
    "rmse_log = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "\n",
    "print(f\"{'Model':<15} {'R²':>10} {'MSE':>15} {'RMSE':>12} {'BP p-value':>15} {'Skewness':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# Simple Linear\n",
    "print(f\"{'Simple Linear':<15} {r2_score(y_test, model.predict(X_test)):10.4f} \"\n",
    "      f\"{mean_squared_error(y_test, model.predict(X_test)):15,.2f} \"\n",
    "      f\"{rmse_simple:12,.2f} \"\n",
    "      f\"{bp_pvalue:15.5f} {skew:10.4f}\")\n",
    "\n",
    "# Quadratic\n",
    "print(f\"{'Quadratic':<15} {r2_score(y_test_poly, y_pred_poly):10.4f} \"\n",
    "      f\"{mean_squared_error(y_test_poly, y_pred_poly):15,.2f} \"\n",
    "      f\"{rmse_quad:12,.2f} \"\n",
    "      f\"{bp_pvalue:15.5f} {skew:10.4f}\")\n",
    "\n",
    "# Log-Transformed (Original Scale)\n",
    "print(f\"{'Log-Transformed':<15} {log_r2:10.4f} \"\n",
    "      f\"{log_mse:15,.2f} \"\n",
    "      f\"{rmse_log:12,.2f} \"\n",
    "      f\"{bp_pvalue_log:15.5f} {stats.skew(residuals_log):10.4f}\")\n",
    "\n",
    "# Uncomment to see log-scale metrics for reference\n",
    "# print(f\"{'Log-Transformed*':<15} {log_r2_logscale:10.4f} \"\n",
    "#       f\"{log_mse_logscale:15,.2f} \"\n",
    "#       f\"{bp_pvalue_log:15.5f} {stats.skew(residuals_log):10.4f}\")\n",
    "# print(\"*Metrics in log scale for reference (not directly comparable)\")\n",
    "\n",
    "print(\"\\nNote: Lower BP p-value indicates more evidence for heteroscedasticity\")\n",
    "print(\"      Skewness closer to 0 indicates more normal residuals\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
